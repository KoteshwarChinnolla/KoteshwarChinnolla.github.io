<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Portfolio Details</title>
    <link href="assets/img/OIP (1).jpeg" rel="icon">
    <link href="assets/css/blogs.css" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">
    <script src="assets/js/commentbox.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="styles.css">
</head>

<body>
    <header>
        <div class="breadcrumbs">
            <a href="index.html">Home</a> / <a href="testblog.html">Blogs</a> / <span>How Do Machine Learn</span>
        </div>
        <h1>How Do Machine Learn ?</h1>
        <p>In this blog, let's understand the machine learning workflow, the different steps to be taken to build a
            machine learning medal from scratch, and the different approaches to perform that specific step. </p>
    </header>
    <div class="dropdown">
        <div class="dropdown-title">Contents</div>
        <div class="dropdown-content">
            <ul>
                <li><a href="#machenelearning">Machine Learning</a></li>
                <li><a href="#machenelearningworkflow">Machine Learning work flow</a></li>
                <li><a href="#dataengineering">Data Engineering</a></li>
                <li><a href="#modeldevelopment">Model development</a></li>
            </ul>
        </div>
    </div>
    <main>
        <section class="portfolio-details">
            <div class="container">
                <div class="main-content">
                    <div class="portfolio-description">
                        <h2 id="machenelearning">Machine learning</h2>
                        <p>Machine learning is a branch of Artificial Intelligence, where a machine learns without
                            explicitly programmed, machine learning concerns the development and study of statistical
                            algorithms that can learn from data and generalize to unseen data, and thus
                            perform tasks without explicit instructions.</p>
                        <p>Machine learning is a set of algorithms. It involves using algorithms to identify hidden
                            patterns in datasets, enabling predictions on new data. </p>
                        <blockquote>
                            <p>a computer’s ability to learn without being explicitly programmed.</p>
                            <cite>by computer scientist Arthur Samuel</cite>
                        </blockquote>
                        <h2 id="machenelearningworkflow">Machine Learning Workflow</h2>
                        <p>For a human the learning basically is reading, understating and answering. These are the
                            three simple steps we as a human follow. Similarly, to make a machine learn, we make the
                            machine to follow the same three simple steps. In machine terms those are collecting data,
                            processing data and making predictions out of it.</p>
                        <p>To make a machine learn from our data, the data needs to be in a machine-understandable way
                            so our first step is data preprocessing, after making the data into machine
                            machine-understandable format we need to treat the data to the machine. For perfect
                            training, we required a perfect model so our next step is to select a perfect model. Now
                            machine is giving the desired results then we have to deploy our model in a real-time
                            environment this is where deployment comes into the picture.</p>
                        <div class="image-container">
                            <img class="gif-fixed-width" src="assets/img/DataScience/machine-learning-1.gif"
                                alt="Machine Learning GIF">
                        </div>
                        <br>
                        <br>
                        <hedding>Lets Make it more simpler</hedding>
                        <h3>Dividing it into three main artifacts</h3>
                        <ul>
                            <li>Data Engineering</li>
                            <br>
                            <li>Model development</li>
                            <br>
                            <li>Deployment</li>
                        </ul>
                        <br>
                        <h2 id="dataengineering">Data Engineering</h2>
                        <blockquote>
                            <p>Data pre-processing increases the quality of the data thus making it fit to be analyzed.
                                It minimizes errors, variations, and duplication, hence enhancing the possibility of
                                getting the right results. </p>
                            <cite>.</cite>
                        </blockquote>
                        <p>As raw data it is noisy, biased, incomplete, unstructured, and missing . so processing raw
                            data may throe errors, such that it is most important to pre process the data. poor data can
                            lead to incorrect prediction, lower accuracy etc.. maintaining the data quality is most
                            impotent to train any model.</p>
                        <div class="dropdown">
                            <div class="dropdown-title">Example</div>
                            <div class="dropdown-content">
                                <p>Why data pre processing is so important ? lets understand with an example , consider
                                    our task is to predict the attendance percentage of student by analyzing the
                                    previous 8 years.</p>
                                <strong>case 1:</strong>
                                <p>let say some students discontinued the school, some of them might skip the school for
                                    first few years.</p>
                                <strong>case 2:</strong>
                                <p>some data entry's are meaning less, over 100%.</p>
                                <strong>case 3:</strong>
                                <p>some data entry's are in the string format where it need to be int.</p>
                                <strong>case 4:</strong>
                                <p>we are having the data in sentences. where machine only understand binary or
                                    numerical data.</p>
                                <p>perfect prediction always demands perfect data, what are the issues with this data ,
                                    case 1 data is Missing, case 2 data contains outlier, case 3 data is invalid, case 4
                                    data is in sentence format, preprocessing is necessary to handle all cases</p>
                            </div>
                        </div>
                        <!-- <p>Why data pre processing is so impotent ? lets understand with an example , consider we are having a students details of attendance percentage of previous 8 years. so our task is to predict the attendance of next year. so we have to collect the data from previous 8 years. let say some students discontinued the school, some of them might skip the school for first few years. so what can be the preprocessing steps that can be performed on that particular data to have a perfect prediction? so, first we have to fill up the missing data(who joined after few years) then to remove the students who are not in the school right now this are necessary steps. Then we needed to convert the names into a machine understanding for mat such as bitty numbers , numerical numbers etc.. this are necessary to mark error less data processing. incomplete data always give errors.  </p> -->
                        <hedding>some of the preprocessing techniques include</hedding>

                        <ul>
                            <li>Cleaning</li>
                            <br>
                            <li>Data Augmentation</li>
                            <br>
                            <li>Dimensionality Reduction</li>
                            <br>
                            <li>Feature Engineering</li>
                            <br>
                            <li>Normalizing</li>
                            <br>
                            <li>Encoding</li>
                            <br>
                            <li>Scaling</li>
                        </ul>
                        <br>
                        <h2>Cleaning</h2>
                        <p>Data cleaning is a processing of filling up missing data, removing outliers or duplication
                            and so on.</p>
                        <al>Missing data</al>
                        <p>Handling with missing data can be done in different methods we can fill up the missing data
                            with mean, median or mode or removing whole record or using machine learning algorithms to
                            fill out those values</p>
                        <ul>
                            <li>mean/median/mode of entire data to fill up missing data</li>
                            <br>
                            <li>removing whole record</li>
                            <br>
                            <li>using machine learning algorithms to fill out missing data</li>
                        </ul>
                        <al>Handling outliers</al>
                        <p>outliers are something that deviates significantly from the rest of the data. which changes
                            the course oof the output</p>
                        <div class="dropdown">
                            <div class="dropdown-title">Example</div>
                            <div class="dropdown-content">
                                <p>presence of out layers cause the prediction to be less accurate, lets understand with
                                    an example we have 5 numbers [1,2,3,4] mean of this number is 2.3, consider we have
                                    [1,2,3,100] and the mean of this data is 26.5. so the mean is deviated from 2.5 to
                                    26.2 the reason being the vale 100 which is acting as a outliers</p>
                            </div>
                        </div>
                        <div class="image-container">
                            <div class="image-item">
                                <img src="assets/img/DataScience/MacheneLearningWorkFlow/outlayers 2.jpg" alt="fig 1">
                                <p class="image-description">fig 1 : Scatter plot</p>
                            </div>
                            <div class="image-item">
                                <img src="assets/img/DataScience/MacheneLearningWorkFlow/boxplot.png" alt="fig 2">
                                <p class="image-description">fig 2 : Box plot</p>
                            </div>
                            <div class="image-item">
                                <img src="assets/img/DataScience/MacheneLearningWorkFlow/outliers_2.png" alt="fig 3">
                                <p class="image-description">fig 3 : Z score</p>
                            </div>
                            <div class="image-item">
                                <img src="assets/img/DataScience/MacheneLearningWorkFlow/Boxplot_vs_PDF.png"
                                    alt="fig 4">
                                <p class="image-description">fig 4 : Box plot vs PDF</p>
                            </div>
                        </div>
                        <al>Scatter plot</al>
                        <p>its a visual representation of the data(fig 1), where we can observe the distribution of the
                            data and detect outliers, sk learn provides some of the libraries to detect the outliers in
                            the scatter plot</p>
                        <p style="color: red">from sklearn.neighbors import LocalOutlierFactor</p>
                        <p style="color: red">from sklearn.ensemble import IsolationForest</p>
                        <al>Box plot(IQR)</al>
                        <p>A boxplot is a standardized way of displaying the dataset based on the <a
                                href="https://en.wikipedia.org/wiki/Five-number_summary">five-number summary :</a> the
                            minimum, the maximum, the sample median, and the first and third quartiles. refer <a
                                href="https://en.wikipedia.org/wiki/Box_plot">wiki</a></p>
                        <p><strong>IQR(Inter Quartile Range) :</strong>It is the difference between first and third
                            quatres. MIN and MAX are calculated based on IQR. we just remove the data points out side
                            MIN and MAX( outliers). fig 2</p>
                        <al>Z score</al>
                        <p>Z score is a statistic that provides a measure of how many standard deviations a data point
                            is away from the mean. statistically most 70% to 90% of the data is present at Mean(between
                            +1 and -1 of standard deviation). so the points far from mean are considered as outliers and
                            can be removed.</p>
                        <p><strong></strong> Z-score normalization</strong> refers to the process of normalizing every
                            value in a dataset such that the mean of all of the values is 0 and the standard deviation
                            is 1</p>
                        <strong>Z score = (value - mean)/standard deviation</strong>
                        <p>Fig 3 shows the standard normal distribution of the data by z score normalization</p>
                        <al>Invalid data</al>
                        <p>Using data standardization, you can identify and convert data from varying formats into a
                            uniform format. in some cases there might be junk values or invalid data in the dataset</p>
                        <strong>This can be removed by : </strong>
                        <p>you can apply standardization techniques to your data after you’ve collected it. This
                            involves developing codes to convert your dirty data into consistent and valid formats.</p>
                        <p>ex : String matching</p>
                        <h4>References</h4>
                        <p><a
                                href="https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba">https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba</a>
                        </p>
                        <p><a href="https://en.wikipedia.org/wiki/Box_plot">https://en.wikipedia.org/wiki/Box_plot</a>
                        </p>
                        <p><a
                                href="https://medium.com/@aashish.singh2k8/how-to-identify-and-remove-outliers-in-your-dataset-a-guide-for-data-scientists-57b07aea7fa8">https://medium.com/@aashish.singh2k8/how-to-identify-and-remove-outliers-in-your-dataset-a-guide-for-data-scientists-57b07aea7fa8</a>
                        </p>
                        <p><a
                                href="hhttps://www.scribbr.com/methodology/data-cleansing">https://www.scribbr.com/methodology/data-cleansing/</a>
                        </p>
                        <br>
                        <br>
                        <h2>Data Augmentation</h2>
                        <p>Data augmentation is the process of artificially generating new data from existing data,
                            primarily to train new machine learning (ML) models. ML models require large and varied
                            datasets for initial training</p>
                        <blockquote>
                            <p>Data augmentation uses pre-existing data to create new data samples that can improve
                                model optimization and generalizability.</p>
                            <cite>IBM</cite>
                        </blockquote>
                        <h3>Why Data Augmentation</h3>
                        <p>Machine Learning and Deep Learning Models relay on large data sets develop accurate models in
                            various contexts. Data Augmentation helps filling out missing values and increase the data
                            size by including variety of data. this helps in:</p>
                        <ul>
                            <li>
                                <p>improving model accuracy</p>
                            </li>
                            <li>
                                <p>Reduced data dependency (not being depending on larger data for training)</p>
                            </li>
                            <li>
                                <p>improving model robustness (Including data in such a way that it include all
                                    situations, such as noise and most unexpected situations to make model trained model
                                    strong)</p>
                            </li>
                            <li>
                                <p>Mitigate overfitting in training data</p>
                            </li>
                        </ul>
                        <p>Data augmentation can be applied when the training data is too small, when there is
                            overfitting in the datasets or if there is lot of missing variables in the datasets. the
                            augumentation can be applies in different areas such as <strong>image Augmentation,Text data
                                Augmentation and Audio Data augmentation </strong></p>

                        <h3>Data augmentation techniques</h3>
                        <dev class="image-container">
                            <div class="image-item">
                                <img src="assets/img/DataScience/MacheneLearningWorkFlow/DataAug.png"
                                    alt="--image title--">
                                <p class="p" class="image-description">Data Augmentation techniques</p>
                            </div>
                        </dev>
                        <al>Image Augmentation</al>
                        <ul>
                            <l>
                                <p><strong>Geometric Transformations</strong> - Scaling, Rotation, Translation,
                                    Shearing, Flipping, etc</p>
                            </l>
                            <l>
                                <p><strong>Color Transformations</strong> - Brightness, Contrast, Saturation, etc</p>
                            </l>
                            <l>
                                <p><strong>Quality Transformations</strong> - Blurring, Sharpening, Edge Detection</p>
                            </l>
                            <l>
                                <p><strong>Arithmetic</strong> - Adding images in different ways(imposing one on anther,
                                    adding it side by side, etcc..)</p>
                            </l>
                        </ul>
                        <div class="image-container">
                            <div class="image-item">
                                <img src="assets/img/DataScience/MacheneLearningWorkFlow/augu1.gif" alt="fig 1">
                                <p class="image-description"> </p>
                            </div>
                            <div class="image-item">
                                <img src="assets/img/DataScience/MacheneLearningWorkFlow/augu2.gif" alt="fig 2">
                                <p class="image-description"></p>
                            </div>
                            <div class="image-item">
                                <img src="assets/img/DataScience/MacheneLearningWorkFlow/augu3.gif" alt="fig 3">
                                <p class="image-description"></p>
                            </div>
                            <div class="image-item">
                                <img src="assets/img/DataScience/MacheneLearningWorkFlow/augu4.gif" alt="fig 4">
                                <p class="image-description"></p>
                            </div>
                            <div class="image-item">
                                <img src="assets/img/DataScience/MacheneLearningWorkFlow/augu5.gif" alt="fig 5">
                                <p class="image-description"></p>
                            </div>
                        </div>
                        <p> all the images over there are just created by using Data augmentation (multiple images using
                            single image), Visit <a href="https://github.com/aleju/imgaug">GitHub repo</a> for some
                            intreating Data augmentation techniques</p>
                        <p>python libraries for data augmentation</p>
                        <ul>
                            <li>
                                <p><strong>TenserFlow/Keras</strong> - Tf.image, Keras preprocessing layers, Keras
                                    ImageDataGenerator, keras.Sequential</p>
                            </li>
                            <li>
                                <p><strong>PyTorch/MxNet</strong> - torchvision.transfroms,
                                    mxnet.gluon.data.vision.transforms</p>
                            </li>
                            <li>
                                <p><strong>Scikit-Learn</strong> - ImageDataGenerator</p>
                            </li>
                            <li>
                                <p><strong>Augmentor</strong></p>
                            </li>
                            <li>
                                <p><strong>Albumentations</strong></p>
                            </li>
                            <li>
                                <p><strong>OpenCV</strong><a
                                        href="https://medium.com/analytics-vidhya/data-augmentation-techniques-using-opencv-657bcb9cc30b"
                                        style="color: #0056b3;"></a></p>
                            </li>
                            <li>
                                <p><strong>AutoAugment (DeepAugment)</strong> <a
                                        href="https://github.com/barisozmen/deepaugment" style="color: #0056b3;">GitHub
                                        repo</a></p>
                            </li>
                            <li>
                                <p><strong>Imgaug</strong> - <a href="https://github.com/aleju/imgaug"
                                        style="color: #0056b3;">GitHub repo</a></p>
                            </li>
                        </ul>
                        <p><a href="https://github.com/NVlabs/DG-Net/tree/master">Image Augmentation all codes</a></p>
                        <p>
                            <a href="https://arxiv.org/pdf/1805.09501"
                                style="color: #0056b3;">https://arxiv.org/pdf/1805.09501</a>
                        </p>
                        <p><a href="https://neptune.ai/blog/data-augmentation-in-python"
                                style="color: #0056b3;">https://neptune.ai/blog/data-augmentation-in-python</a></p>
                        <p><a href="http://ai.stanford.edu/blog/data-augmentation/"
                                style="color: #0056b3;">http://ai.stanford.edu/blog/data-augmentation/</a></p>
                        <p><a href="https://www.datacamp.com/tutorial/complete-guide-data-augmentation"
                                style="color: #0056b3;">https://www.datacamp.com/tutorial/complete-guide-data-augmentation</a>
                        </p>
                        <p><a href="https://github.com/aleju/imgaug"
                                style="color: #0056b3;">https://github.com/aleju/imgaug</a></a></p>
                        <p><a href="https://github.com/barisozmen/deepaugment"
                                style="color: #0056b3;">https://github.com/barisozmen/deepaugment</a></a></p>
                        <p><a href="https://github.com/keras-team/keras-preprocessing"
                                style="color: #0056b3;">https://github.com/keras-team/keras-preprocessing</a></a></p>
                        <p><a href="https://neptune.ai/blog/data-augmentation-in-python"
                                style="color: #0056b3;">https://neptune.ai/blog/data-augmentation-in-python</a></p>
                        <p></p>
                        <al>Text Augmentation</al>
                        <p>Text augmentation is the process of artificially generating new text from existing text,
                            primarily to train machine learning (ML) models or NLP models. The fields like computer
                            vision and natural language processing require large and varied datasets for initial
                            training. Making the dataset more robust and generalizable improves the performance of the
                            model. There are various text augmentation techniques that can be applied to improve the
                            performance of the model. Majorly text augmentation input replacing , adding, removing,
                            shuffling, etc. lets see some of the text augmentation techniques.</p>
                        <SS>Most widely used text augmentation techniques are as follows</SS><br><br>
                        <dev class="image-container">
                            <div class="image-item">
                                <img src="assets/img/DataScience/MacheneLearningWorkFlow/textaug.png"
                                    alt="--image title--">
                                <p class="p" class="image-description">fig 1 : Artificial Neural Network</p>
                            </div>
                        </dev>
                        <SS>Word2vec-based augmentation</SS>
                        <p>-- (Semantic Similarity Augmentation) the word is replaced with the word with close value in
                            the embeddings. this required a pretrained word embedding models or the data is big enough
                            to build a embedding model. implemented by fasttext and word2vec</p>
                        <SS>WordNet-based augmentation</SS>
                        <p>-- (Synonym replacement) this approach divides the sentence into verbs and nouns and replaces
                            the nouns with their respective synonyms. the synonyms are generated using WordNet(a popular
                            lexical database).</p>
                        <SS>RTT-based augmentation</SS>
                        <p>(Round Trip Translation RTT) this approach takes a sentence and translates it into another
                            language and then translates it back to the original language. this can be implemented using
                            textaugment or google translate api.</p>
                        <SS>Easy data augmentation (EDA)</SS>
                        <p>This include some of the operations such as Synonym replacement, random deletion,random
                            insertion, random swap, etc </p>
                        <SS>An easier data augmentation (AEDA)</SS>
                        <p>Random insertion of punctuations and special characters</p>
                        <SS>Mixup augmentation</SS>
                        <p>The technique is quite systematically named. We are literally mixing up the features and
                            their corresponding labels. we are creating new virtual dataset from the training data set
                            by mixing up the features and their corresponding labels. the implementation is basically
                            creating a NN model making it generate a new input and a new output labels.</p>
                        <p><a href="https://github.com/makcedward/nlpaug">nlpaug GitHub repo</a></p>
                        <p><a href="https://keras.io/examples/vision/mixup/">MixUp</a></p>
                        <p><a href="https://arxiv.org/pdf/1907.03752">Text Augmentation</a></p>
                        <p><a href="https://pypi.org/project/textaugment/">PyPy textaugment</a></p>
                        <p><a href="https://github.com/dsfsi/textaugment/tree/master">Implementation</a></p>
                        <al>Audio Augmentation</al>
                        <p>Audio recognition systems such as Google assistant, Alexa, Amazon Echo etc.. are developed on Audio data and detecting more complex audios is a challenge. as people are more interested into voice assistants it is more important to detect the audio form most complex environments such as noisy, low audio, multiple audios etc.. The systems only achieve this complex tasks by making it to train through such data sets. But collecting such datasets needs lot of human effort. Hence this is where data augmentation comes to rescue. By using this techniques we can build more robust and large datasets to make detection more accurate.</p>
                        <SS>Audio Augmentation techniques</SS>
                        <ul>
                            <li>Noise injection</li>
                            <li>Time shift</li>
                            <li>Time stretching</li>
                            <li>Random cropping</li>
                            <li>Pitch scaling</li>
                            <li>Dynamic range compression</li>
                            <li>Simple gain</li>
                            <li>Equalization</li>
                            <li>Voice conversion (Speech)</li>
                        </ul>
                        <SS>Background noice</SS><br>
                        <div class="image-container">
                            <div class="image-item">
                                <audio controls>
                                    <source src="assets/img/DataScience/MacheneLearningWorkFlow/AddBackgroundNoise_input.flac" type="audio/flac">
                                    Your browser does not support the audio element.
                                </audio>
                                <p class="image-description">Fig 1: original</p>
                            </div>
                            <div class="image-item">
                                <audio controls>
                                    <source src="assets/img/DataScience/MacheneLearningWorkFlow/AddBackgroundNoise_transformed.flac" type="audio/flac">
                                    Your browser does not support the audio element.
                                </audio>
                                <p class="image-description">Fig 1: Transformed</p>
                            </div>
                        </div>
                        <SS>Gaussian noise</SS><br>
                        <div class="image-container">
                            <div class="image-item">
                                <audio controls>
                                    <source src="assets/img/DataScience/MacheneLearningWorkFlow/AddGaussianNoise_input.flac" type="audio/flac">
                                    Your browser does not support the audio element.
                                </audio>
                                <p class="image-description">Fig 1: original</p>
                            </div>
                            <div class="image-item">
                                <audio controls>
                                    <source src="assets/img/DataScience/MacheneLearningWorkFlow/AddGaussianNoise_transformed.flac" type="audio/flac">
                                    Your browser does not support the audio element.
                                </audio>
                                <p class="image-description">Fig 1: Transformed</p>
                            </div>
                        </div>
                        <p><a href=https://iver56.github.io/audiomentations>audiomentations</a>-- this is a library provided by python</p>
                        <p><a href="https://www.tensorflow.org/io/tutorials/audio">TenserFlow</a></p>
                        <p><a href="https://pytorch.org/audio/2.0.1/tutorials/audio_data_augmentation_tutorial.html">PyTorch</a></p>
                        <p><a href="https://www.mathworks.com/help/audio/ref/audiodataaugmenter.html">Using MathLab</a></p>
                        <al>Time Series Data Augmentation </al><br>
                        <p>Time Series Data is basically known as sequence of data points over a period of time. We basically observe this kind of data in Stock market and IOT applications. Treating this data to be Augmented as the previous three techniques cause problem. Because previous every record is concorded as the individual data points. But we need to treat a sequence of data points as a single data point in timeseries. Time series data augmentation follows its suppurate techniques. Such as GAN(Generative Adversarial Network) and Autoencoders. </p>
                        <p><SS>Traditional algorithms</SS> this techniques can tbe applied directly to time series data. instead we divide the entire data into a group of time stramps. then applying then applying to one of them or some of them and then adding it to original data set results in increase in size of data.. it can also be said that some of them does not support so being selective would be effective.</p>
                        <ul>
                            <li>time scaling window</li>
                            <li>jittering</li>
                            <li>Rotation</li>
                            <li>Channel permutation</li>
                            <li>wrapping</li>
                        </ul>
                        <div class="image-container">
                            <div class="image-item">
                                <img src="assets/img/DataScience/MacheneLearningWorkFlow/GAN.png" alt="fig 1">
                                <p class="image-description">GAN(Generative Adversarial Network)</p>
                            </div>    
                            <div class="image-item">
                                <img src="assets/img/DataScience/MacheneLearningWorkFlow/AutoEncoder.png" alt="fig 1">
                                <p class="image-description">Virtual autoencoder</p>
                            </div>
                        </div>
                        <al>GAN(Generative Adversarial Network)</al>
                        <p>Gan architecture is based on the computation between two Neural Networks known to be generator and discriminator. the generator is used to generate fake data and the discriminator is used to differentiate between real and fake data. the whole task is to fool the discriminator. in this way we have more data points. in case of time series data, we can use this technique to generate data points that are similar to the original data points for a particular period of time.</p>
                        <al>Virtual autoencoder</al>
                        <p>This is a Encoder and decoder architecture. Majorly used in unSupervised learning where there is no labels. So here we use the encoder to make the input datapoints into smaller dimensions and decoder to make it back to original. so here we are basically generating new data points by making it generate back the original data.</p>
                        <p><a href="https://arxiv.org/pdf/2206.13508">Time series data augmentation</a></p>
                        <p><a href="https://github.com/jsyoon0823/TimeGAN/tree/master?tab=readme-ov-file"> Code github repo</a></p>
                        <p><a href="https://saturncloud.io/glossary/data-augmentation-with-generative-ai/">Gen AI Data Augmentation</a></p>
                        <p><a href="https://aws.amazon.com/what-is/gan/">AWS GAN</a></p>
                        <h2>Dimensionality reduction</h2>
                        <P>Dimensionality reduction is a technique to reduce the number of feature in the dataset. Basically higher the number of feature slower the model will be and model leads to overfitting. the data fits well on the training data but fails to generalize well on new or testing data. The main aim of the Dimensionality reductions is get more information out of the entire data into fewer features and also to remove the features that has no/less effect on the model. We apply Dimensionality reduction to make model faster ,reduce complexity and improve performance.</P>
                        <p>Dimensionality reduction is a technique which selects or extracts completely independent features that has the maximum impact on the model.</p>
                        <p>Having more features results in high dimensional visualization which is hard to interpret and generalize. which may results in complex model formation</p>
                        <p> For Dimensionality reduction there are two main approaches  <SS>Feature Extraction</SS>,<SS>Feature Selection</SS></p>
                        <p><SS>Feature Extraction : </SS> It involves creating new features from original by combining or transforming exiting features. the ogle is to create a set of d feature form n features (d<=n) such that d features capture the essence of the n features</p>
                        <p>Some of the feature extraction techniques are as follows<SS>Principal Component Analysis(PCA),Singular Value Decomposition(SVD),Linear Discriminant Analysis(LDA) etc..</SS></p>
                        <p><SS>Feature Selection : </SS> It involves selecting a subset of features from the original set of features. the goal is to select a subset of features that are most relevant and can have the maximum information content</p>
                        <p>Some of the feature selection techniques are <SS>Filter methods</SS>,<SS>Wrapper methods</SS>,<SS>Embedding method</SS></p>
                        <div class="image-container">
                            <div class="image-item">
                                <img src="assets/img/DataScience/MacheneLearningWorkFlow/DR1.png" alt="fig 1">
                                <p class="image-description">Dimensionality reduction</p>
                            </div>    
                        </div>
                        <al>Principal component analysis (PCA)</al>
                        <p>Principal component analysis is a Feature Extraction technique where Higher dimensional data is transformed into lower dimensional data with the most important information. here we are using covariance matrix to higher dimensional and getting eigen vector and racking the eigen values to select the eigen vectors . this is most effective reduction technique as it uses the covariance matrix which gives the correlation between the feature. </p>
                        <p>Before we get started, we shall take a quick look at the difference between covariance and variance. Variance measures the variation of a single random variable (like the height of a person in a population), whereas covariance is a measure of how much two random variables vary together (like the height of a person and the weight of a person in a population). The formula for variance is given by</p>
                        <p>$$\sigma^2_x = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2$$</p>
                        <p>and the formula for covariance is given by</p>
                        <p>$$cov(x,y) = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})$$</p>
                        <p><SS>Covariance : </SS>where x and y are two random variables, n is the number of observations, and x_i and y_i are the values of the random variables at observation i.so we are considering two features x and y from a set of features in the dataset and finding covariance between each pair of features which represents how this features vary together. <a>$$\text{Covariance varies between } -\infty \text{ and } \infty$$</a> which means cov(x,y) > 0 employs features related in same direction, cov(x,y) < 0 represents features releated in opposite direction and cov(x,y)=0 represents independent features. if the number of fetures are n then we form the matrix of size n x n by finding the covarince between each pair.</p>
                        <p>$$\text{Covariance matrix}=\begin{bmatrix} \sigma_{xx} &amp; \sigma_{xy} \\ \sigma_{yx} &amp; \sigma_{yy} \end{bmatrix}$$</p>
                        <p><a href="https://byjus.com/maths/covariance/">know more</a></p>
                        <p><SS>Eigan values and Eigan vectors :</SS>Here we use Eigan values and Eigan vectors to reduce the Dimensionality of the covariance matrix. It helps in finding the important information out of all features in lower dimensional space. ⋋ represents Eigan value and X represents Eigan vector. ⋋ ranks the Eigan vectors such that higher the rank the more information the Eigan vector posses, Eigan vectors are the set of independent vectors.</p>
                        <p>$$Ax=\lambda x$$</p>
                        <p>$$(A-\lambda I)x=0$$</p>
                        <p>$$det(A-\lambda I)=0$$</p>
                        <p> we can find the eigen values and eigen vectors using this mathematically, for more info <a href="https://byjus.com/jee/eigenvalues-and-eigenvectors-problems-and-solutions/">Vist</a></p>
                        <p><SS>Programmatically</SS></p>
                        <div class="code-box" id="codeBox"><button class="copy-btn" onclick="copyCode()">Copy</button>
                            <pre><code  class="language-python">import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Sample data
data = {
    'Feature1': [2.5, 0.5, 2.2, 1.9, 3.1, 2.3, 2.0, 1.0, 1.5, 1.1],
    'Feature2': [2.4, 0.7, 2.9, 2.2, 3.0, 2.7, 1.6, 1.1, 1.6, 0.9],
    'Feature3': [100, 102, 98, 101, 97, 105, 100, 104, 101, 102]
}
df = pd.DataFrame(data)

# Standardizing the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df)

# Applying PCA
pca = PCA(n_components=2)
principal_components = pca.fit_transform(scaled_data)

# Creating a DataFrame with the principal components
pca_df = pd.DataFrame(data=principal_components, columns=['Principal Component 1', 'Principal Component 2'])

print(pca_df)</code></pre></div>
                        
                        <p><SS>Standardizing</SS> basically transforms the data into a normal distribution, such that every feature has a mean of 0 and a standard deviation of 1 (making the values closer to 0), this ensures all features contribute equally. the features with large or small values are normalized ensuring every feature is on the same scale. <SS>This step is very essential before applying PCA</SS></p>
                        <p> Here 3 features are transformed to 2 features by selecting n_components=2 as a hyperparameter.</p>
                        <p></p>
                        <h2>Model development</h2>
                        <p></p>
                        <p>.</p>
                        <p>.</p>
                        <p>.</p>
                        <h2>Deployment</h2>
                        <p>.</p>
                        <p>.</p>
                        <p>.</p>
                        <p>.</p>
                    </div>
                </div>
                <aside class="comment-box">
                    <div class="project-info">
                    <al>Post your comment here</al><br><br>
                    <SS>Your comments are most valuable.Please be responsible, everyone can view your comments 📩</SS><br><br>
                    <input type="hidden" id="blog-id" value="1">
                    <input type="text" id="name-input" style="height: 40px; width: 95%; font-size: 20px; border: 1px solid #c4b2b2; border-radius: 4px; padding: 8px; margin-bottom: 8px; font-family: Arial, sans-serif; font-size: 14px; " placeholder="Your Name" />
                    <textarea id="suggestion-input" placeholder="Enter your comment here..."></textarea>
                    <button type="submit" onclick="submitComment()">Submit</button>
                    <ul id="comments-list" class="comments-list"></ul>
                </aside>
            </div>
        </section>
    </main>

    <footer>
        <a href="#top" class="scroll-top">🔝</a>
    </footer>
</body>

</html>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
<script>
        document.addEventListener("DOMContentLoaded", () => {
        const dropdowns = document.querySelectorAll(".dropdown");

        dropdowns.forEach(dropdown => {
            const title = dropdown.querySelector(".dropdown-title");

            title.addEventListener("click", () => {
                // Toggle active class for the clicked dropdown
                dropdown.classList.toggle("active");

                // Optionally close other dropdowns (uncomment below)
                // dropdowns.forEach(d => {
                //     if (d !== dropdown) d.classList.remove("active");
                // });
            });
        });
    });
    function copyCode() {
        const codeBox = document.querySelector("#codeBox code");
        const codeText = codeBox.innerText.trim(); // Get the code text
        navigator.clipboard.writeText(codeText)
            .then(() => alert("Code copied to clipboard!"))
            .catch(err => alert("Failed to copy code: " + err));
    }
    document.addEventListener("DOMContentLoaded", () => {
    const dropdowns = document.querySelectorAll(".dropdown");

    dropdowns.forEach(dropdown => {
        const title = dropdown.querySelector(".dropdown-title");

        title.addEventListener("click", () => {
            // Toggle active class for the clicked dropdown
            dropdown.classList.toggle("active");

            // Optionally close other dropdowns (uncomment below)
            // dropdowns.forEach(d => {
            //     if (d !== dropdown) d.classList.remove("active");
            // });
        });
    });
});

</script>