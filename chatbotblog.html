<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Portfolio Details</title>
    <link href="assets/img/OIP (1).jpeg" rel="icon">
    <link href="assets/css/blogs.css" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <div class="breadcrumbs">
            <a href="index.html">Home</a> / <a href="testblog.html">Blogs</a> / <span>Chatbot</span>
        </div>
        <h1>YOUR COMPANION - an AI chatbot</h1>
        <p>In this blog,  Let's understand how i built the chatbot and deployed it into AWS cloud</p>
    </header>
    <div class="dropdown">
        <div class="dropdown-title">Contents</div>
        <div class="dropdown-content">
            <ul>
                <li><a href="#introduction">Introduction</a></li>
                <li><a href="#Langchain&AmazonTitan">Langchain and Amazon Titan</a></li> 
                <li><a href="#fastAPI">Fast API</a></li>
                <li><a href="#Docker">Docker Image and container</a></li>
                <li><a href="#modeldevelopment">Model development</a></li>
              </ul>        
            </div>
    </div> 
    <main>
        <section class="portfolio-details">
            <div class="container">
                <div class="main-content">
                    <div class="portfolio-description">
                        <h2 id="machenelearning">Introduction</h2>
                        <p>Your companion is an AI chatbot, which is built for a portfolio website. the working of chatbot involve gathering the user information by asking the questions like "your good name?" and "describe your purpose?", then information is sent AWS Dynamodb through API GateWay and AWS Lambda. then the additional Questions from the user with the information user provided initially which is also is stored in a short term memory is then sent as input to Amazon titan LLM From Bedrocks by using Langchain. then LLM process process the input then response is displayed on the chatbot.</p>
                        <p>Every thing until this is fine and works well on our local system. but real challenge occurs when we need to present the same thing to public Webpage. so here is where the Aws comes into picture and we need to deploy our chatbot in a real-time environment. firstly AWS Bedrocks and Langchain needs to be implemented with Fast API. then here comes Docker where a Docker image is bult for Fast API application . then this Docker Image is deployed into EC2 instance which is running on AWS. the public url given by EC2 then integrated to API gateway so we can push and get response. then API gateway deployed URL is then used in frontend application, to get responses. coming to the data storage this is basically through AWS Lambda and Dynamodb. Where API gateway link is generated to AWS Lambda and DynamoDB integration and used in frontend application.</p>
                        <div class="image-container">
                            <div class="image-item">
                                <img src="assets/img/chatbotblog/chatbotworkflow.png" alt="fig 1">
                                <p class="image-description">fig 1 : work flow</p>
                            </div>
                        </div>
                        <h2 id="Langchain&AmazonTitan">Langchain and Amazon Titan</h2>
                        <p>Langchain is a framework that allows users to build applications with LLMs. It's collection of tools simples building generative AI applications featuring access to multiple LLMs like OpenAI, Huggingface, Bedrocks, Lalma etc.</p>
                        <h4>Features of Langchain</h4>
                        <ul>
                            <li><p>Model interaction</p></li>
                            <li><p>Data storage and retrieval</p></li>
                            <li><p>Memory</p></li>
                            <li><p>Agents</p></li>
                            <li><p>chains</p></li>
                        </ul>
                        <a href="https://github.com/langchain-ai/langchain"><p style="color: #007bff;">Have a look at this for more information</p></a>
                        <h4>Role of langchain</h4>
                        <p>As Lanchain simplifies accesing llm's, such that i am using langchain to access <strong style="color: #007bff;"><a href="https://aws.amazon.com/bedrock/">Bedrocks</a></strong> . Bedrocks is a service provided by AWS where we host and find pretrained LLM's. we are using <strong style="color: #007bff;"><a href="https://aws.amazon.com/bedrock/amazon-models/titan/">Amazon Titan</a></strong> as our LLM for this project. AmazonTitan is a popular LLM which is Trained on wide range of data, we can use it as out GPT. For a over view Langchin allows you to access LLM/s present in the Bedrocks so we are choosing Amazon Titan.</p>

                        <div class="code-box" id="codeBox"><button class="copy-btn" onclick="copyCode()">Copy</button>
<pre><code  class="language-python">from langchain.llms.bedrock import Bedrock

model = Bedrock(
    credentials_profile_name="default",  # AWS credentials profile
    region_name="us-east-1",
    model_id="amazon.titan-text-premier-v1:0",  # Amazon Bedrock model
    model_kwargs={
        "temperature": 0.9,
    },
)
response = custom_llm.invoke(input="What is the recipe of mayonnaise?")
print(responce)
</code></pre></div>
                        <p>Som people might not have AWS account and find it vary difficult to access Bedrocks. so we can also use <a href="https://huggingface.co/blog/langchain"><strong style="color: #007bff;">Huggingface & Langchain</strong></a> intigration this is more simpler approach or we canalso impliment Some of the populer LLMs like OpenAi and Lalma directly from langchiain. vist <a></a> directly from here <a href="https://github.com/langchain-ai/langchain/tree/master/docs/docs/integrations/llms"><strong style="color: #007bff;">Langchain Intigrations</strong></a></p>
                        <p>Make sure you visit <a href="https://github.com/langchain-ai/langchain/tree/master/docs/docs/integrations/llms" style="font-size: 25px; color: #007bff;">Here</a>. every integration is documented</p>
                        <p>**Before you run the code AWS credentials needs to be connected, the reason behand <strong>credentials_profile_name=default</strong> is the credentials are already stored in a default profile and to do that follow <a href="https://medium.com/@simonazhangzy/connect-vs-code-to-aws-87e274e5cd4" style="color: #007bff; font-size: 20px;">this steps</a> this can also help you if you what to access more bedrocks models</p>
                        <p><strong> model_id="amazon.titan-text-premier-v1:0"</strong> i am using Amazon Titan but you can use any other LLMs. you just need to change the model_ID and a list of different model ids are available in <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html" style="font-size: 20px; color: #007bff;">Here</a> you can chose from pool of different LLMs.</p>
                        <p>model_kwargs are Hyper parameters for the model. where we can able adjust the temperature, max_tokens, min_tokens.stop_sequence etc..</p>
                        <p><SS>custom_llm.invoke(input="What is the recipe of mayonnaise?")</SS> you can change the input as you want to get the response accordingly.</p>
                        <h2 id="fastAPI">FAST API</h2>
                        <blockquote>
                            <p>FastAPI is a modern, high-performance web framework for building APIs with Python 3.7+ based on standard Python type hints</p>
                            <cite>FastAPI</cite>
                        </blockquote>
                        <p>as we seen earlier we are using LangChain and Titan integration get the responses to our prompts which are sent using python. But for any Realtime projects it has to be sent through an webpage but though webpage we cant directly access this all code manually. as webpage uses HTML/CSS and we use python for communication. this is where <SS>API</SS> comes into picture. API's can be accessed through any platforms such as HTML/JS/react to post and get the information. API is not a code it is a link that can be accessed easily.</p>
                        <p>I am using FastAPI for building an API for the Langchain & Bedrocks integration. if we give the input as a prompt to the API then it will give us back the output. this pigmentation is done using <SS>Fast API</SS>.</p>
                        <p><SS>Fast API</SS> gives you lot of flexibility to build high performing API's in user most easy implementation techniques for production environments. </p>
                        <div class="code-box" id="codeBox"><button class="copy-btn" onclick="copyCode()">Copy</button>
                            <pre><code  class="language-python">from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from langchain.llms.bedrock import Bedrock
import uvicorn


app = FastAPI()


app.add_middleware(
    CORSMiddleware,#0563bb90
    allow_origins=["*"],  # Replace "*" with specific allowed origins in production
    allow_credentials=True,
    allow_methods=["*"],  # Allow all HTTP methods, including OPTIONS
    allow_headers=["*"],  # Allow all headers
)


def demo_chatbot():
    model = Bedrock(
        credentials_profile_name="default",  # AWS credentials profile
        region_name="us-east-1",
        model_id="amazon.titan-text-premier-v1:0",  # Amazon Bedrock model
        model_kwargs={
            "temperature": 0.9,
            # "max_tokens_to_sample": 50,
            # "stop": ["\n\n\n"]
        },
    )
    return model

# Define the request schema
class ChatRequest(BaseModel):
    message: str

# Define the chat endpoint
@app.post("/chat")
async def chat(chat_request: ChatRequest):
    print(chat_request)
    user_message = chat_request.message
    if not user_message.strip():
        raise HTTPException(status_code=400, detail="Message cannot be empty.")

    try:
        # Generate a response from the chatbot model
        model = demo_chatbot()
        response = model.predict(user_message)
        return {"reply": response}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"An error occurred: {str(e)}")

# Health check endpoint
@app.get("/")
async def read_root(request: Request):
    return {"message": "Hello, World!"}



if __name__ == "__main__":
    uvicorn.run(app, host="127.0.0.1", port=5000) </code></pre></div>
                        <p><SS>demo_chatbot</SS> is a function used to access the <SS>AmazonTitan</SS> model. which is previously explained already</p>
                        <p> as you run the code and your server is running on  <SS>http://127.0.0.1:5000/</SS> and it is the home address of your server, but your chat bot is actually running on <SS>http://127.0.0.1:5000/chat</SS> where you can post the request and get the response. because as you look at the code the function demo_chatbot() is being called in the route <SS>@app.post("/chat")</SS> and this route is handling the POST request.</p>
                        <p><ss>app.add_middleware</ss> this is one of the most important step when you are using the API for real time deployment. because it allows all the origins, methods and headers. it basically allows you to make cross-origin requests.</p>
                        <p><SS>ChatRequest(BaseModel)</SS> pydantic is one of the Special library that is used to validate the data that is being sent to the API. that means it make sure that the data is in the correct format. in our case it has to be string.</p>
                        <p><SS>async def</SS> if you would like to know the difference the difference between def and async def , its vary simple async def parallelly process the requests you make and def serially process the requests. understand it with an - <a href="https://fastapi.tiangolo.com/async/"><al> interesting example</al> </a>. and rest is just behave as a function witch return the model response by calling demo_chatbot</p>
                        <div class="image-container">
                            <div class="image-item">
                                <video controls>
                                    <source src="assets/img/chatbotblog/fastapi.mp4" type="video/mp4">
                                    Your browser does not support the video tag.
                                </video>
                                <p class="image-description">Follow the steps</p>
                            </div>
                        </div>
                        <SS>This is one way of testing your Fast API, else you can also follow this code to test if the API is perfectly running</SS>
                        <div class="code-box" id="codeBox"><button class="copy-btn" onclick="copyCode()">Copy</button>
<pre><code  class="language-python">import requests
url = 'http://127.0.0.1:5000/chat/'
data = {'message': "hello, how are you"}
# headers = {'Content-type': 'application/json'}
response = requests.post(url, json= data)
print(response.content)
</code></pre></div>
                        <p>If You could get a response, that means your Fast API is working fine and you built an API for your local system. you could not share the link/IP you got by running the python code. because it is your local computer IP address. just you can access because it is running on your local system. to make it access by public computer it needs to be running on the cloud. to do this we need to dockerize the fast api file and deploy it on the cloud and make it run always. so that every one can access it.</p>
                        <h2 id="Docker">Docker Image & Container</h2>
                        <blockquote>
                            <p>Docker is a set of platform as a service products that use OS-level virtualization to deliver software in packages called containers. </p>
                            <cite>Wikipedia</cite>
                        </blockquote>
                        <br>
                        <p>lets simplify it, so as i said you when you are running a python code it gives you the output according to what ever function your python code is written for , it might be regarding a machine learning library(sciket learn), or a numpy, pandas for data processing. so how this is done, fist if you want to use a machine learning library you need to install it on to your machine then write a python code to give the input and get the output. that is how you can perform functions on your machine. But when it comes to real time deployment you are not only representing it on your machine but also public machines, but we dont know if the public machine has that particular library installed. so it is our task to make sure that every one could get the results. this is when docker comes into picture, docker provides you a virtual machine (os, this is the machine that is hosted on the cloud) where you can install all the requirements and the instructions to get the outputs when they give the input. you just need to ask the Virtual Machine that is hosted on the cloud.</p>
                        <p><SS>What are this instructions ?</SS> Instructions are basically the commands you are writing on a dockerFile. that include to use this particular base image (in our case python), to install some set of library's, to run some code and lot more .</p>
                        <a href="https://docs.docker.com/reference/dockerfile/">All Instructions</a>
                        <p><ss>Docker file</ss> is a text file that contains instructions to build an image. Instructs are what all include and what all operations to perform.</p>
                        <a href="https://blog.devopscommunity.in/dockerfile-simplified-how-to-make-your-own-dockerfile">Dockier File format</al></a>
                        <p><SS>Docker Image</SS> is a template that contains the instructions to build a container. we build a image through the instructions. image contain all the installed packages all the files to be executed.</p>
                        <p><SS>Container</SS> is a running instance of a docker image. it will do what all our python code needs to be done.</p>
                        <SS>Make sure your files are arranged in this way</SS>
                        <div class="code-box" id="codeBox"><button class="copy-btn" onclick="copyCode()">Copy</button>
                        <pre><code  class="language-python">project-root/
    ├── app/
    │   └── app.py
    ├── Aws/
    │   ├── config.json
    │   ├── credentials.json
    ├── requirements.txt
    ├── .dockerignore
    └── Dockerfile  
                            </code></pre></div>
                            <p>requirements.txt can be etracted by running <SS>pip freeze > requirements.txt</SS> command in your project directory</p>
                            <center><SS style="color: #007bff;">Requirements.txt</SS><SS> --    for this project this are all you need to be install</SS></center>
                            <div class="code-box" id="codeBox"><button class="copy-btn" onclick="copyCode()">Copy</button>
                                <pre><code  class="language-python">fastapi==0.95.1
uvicorn[standard]==0.22.0
pydantic==1.10.2
langchain[community]==0.0.196
boto3==1.28.57
botocore==1.31.57</code></pre></div>
                            <center><h3 style="color: #007bff;">.dockerignore</h3>-<SS>this are some thing that has to be ignore while creating the image</SS></center>
                            <div class="code-box" id="codeBox"><button class="copy-btn" onclick="copyCode()">Copy</button>
                                <pre><code  class="language-python">__pycache__
*.pyc
*.pyo
*.pyd
*.db
*.sqlite3</code></pre></div>
                            <p>What ever the now the fast api code is written it will be stored in app.py. and one change need to make <SS>"replace host="127.0.0.1" with host="0.0.0.0""</SS> as we need to deploy it for production environment. 0.0.0.0 is the public IP address which allow traffic from any IP address.</p>

                            <center><h3 style="color: #007bff;">Dockerfile</h3></center>
                            <div class="code-box" id="codeBox"><button class="copy-btn" onclick="copyCode()">Copy</button>
                            <pre><code  class="language-python"># Use a base Python image
FROM python:3.9-slim

# Set environment variables
ENV PYTHONUNBUFFERED 1
ENV PYTHONDONTWRITEBYTECODE 1

# Set the working directory inside the container
WORKDIR /code

# Copy the requirements file into the container
COPY requirements.txt .

# Install project dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Copy the rest of the application code into the container
COPY . .

COPY .aws /root/.aws
# Expose the port your FastAPI app will run on
EXPOSE 5000

# Command to run your FastAPI app with uvicorn
CMD ["uvicorn", "app.app:app", "--host", "0.0.0.0", "--port", "5000"]</code></pre></div>
                        <p><SS>FROM python:3.8-slim</SS> This command sets the foundation for the build. python:3.8-slim is a lightweight version of the Python 3.8 image, optimized for size and speed. Using this slim image reduces the overall size of your Docker image, leading to quicker downloads and less surface area for security vulnerabilities. This is particularly useful for a Python-based application where you might not need the full standard Python image.</p>
                        <p><SS>ENV PYTHONUNBUFFERED 1</SS> This command sets the environment variable PYTHONUNBUFFERED to 1. This is often used in Docker containers to ensure that Python's output is not buffered, which can lead to unexpected behavior.</p>
                        <p><SS>WORKDIR /code</SS> now code is our current working directory, what ever we copy will go into folder code</p>
                        <p><SS>COPY requirements.txt .</SS> copy the requirements file into the current working directory</p>
                        <p><SS>RUN pip install --no-cache-dir -r requirements.txt</SS> install the dependencies from the requirements file. --no-cache-dir is used to avoid caching the dependencies in the container. -r is used to specify the requirements file path.</p>
                        <p><SS>COPY . .</SS> copy the rest of the application code into the current working directory</p>
                        <p><SS>COPY .aws /root/.aws</SS> copy the aws credentials into the containers root directory( which is environment variable)</p>
                        <p><SS>EXPOSE 5000</SS> expose the port 5000 which is used by the FastAPI app</p>
                        <p><SS>CMD ["uvicorn", "app.app:app", "--host", "0.0.0.0", "--port", "5000"]</SS> command to run your FastAPI app with uvicorn</p>
                        <p>Install Docker Desktop if you don't have it. make sure you open the docker application to make the engine work</p>
                        <div class="code-box" id="codeBox"><button class="copy-btn" onclick="copyCode()">Copy</button>
                            <pre><code  class="language-python">docker build -t "name of your choice" .</code></pre></div>
                            <p>. represents the current directory where the Dockerfile is located</p>
                            <div class="code-box" id="codeBox"><button class="copy-btn" onclick="copyCode()">Copy</button>
                                <pre><code  class="language-python">docker container run --name "name container" -d -p 5000:5000 "image name"</code></pre></div>
                            <p>port needs to be same as what you specified in your docker file</p>
                            <SS>if you open docker desktop you will see your container running, this is the indication of successful deployment of docker image. this can be used for production environment.</SS>
                    </div>
                </div>
                <aside class="portfolio-info">
                    <hedding>Project Information</hedding>
                    <br>
                    <br>
                    <ul>
                        <li><strong>Name:</strong>How do machine learn</li>
                        <li><strong>writer:</strong>Chinnolla Koteshwar</li>
                        <li><strong>Date:</strong> 07 December, 2024</li>
                        <li><strong>URL:</strong> <a href="https://koteshwarchinnolla.github.io/portfolio-koteshwar/">koteshwar portfolio</a></li>
                    </ul>
                    <a href="index.html" class="visit-button">Visit Website</a>
                    <br>
                    <br>
                    <br>
                    <div class="suggestion-box">
                        <h3>Leave a Suggestion</h3>
                        <textarea placeholder="Enter your suggestions here..."></textarea>
                        <button type="submit" class="submit-button">Submit</button>
                    </div>
                </aside>
            </div>
        </section>
    </main>

    <footer>
        <a href="#top" class="scroll-top">🔝</a>
    </footer>
</body>
</html>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
<script>
    function copyCode() {
        const codeBox = document.querySelector("#codeBox code");
        const codeText = codeBox.innerText.trim(); // Get the code text
        navigator.clipboard.writeText(codeText)
            .then(() => alert("Code copied to clipboard!"))
            .catch(err => alert("Failed to copy code: " + err));
    }
    document.addEventListener("DOMContentLoaded", () => {
    const dropdowns = document.querySelectorAll(".dropdown");

    dropdowns.forEach(dropdown => {
        const title = dropdown.querySelector(".dropdown-title");

        title.addEventListener("click", () => {
            // Toggle active class for the clicked dropdown
            dropdown.classList.toggle("active");

            // Optionally close other dropdowns (uncomment below)
            // dropdowns.forEach(d => {
            //     if (d !== dropdown) d.classList.remove("active");
            // });
        });
    });
});

</script>